# llm configuration (openai-compatible format)
# for ollama: http://localhost:11434/v1/chat/completions
# for openai: https://api.openai.com/v1/chat/completions
# for anthropic: https://api.anthropic.com/v1/messages
LLM_ENDPOINT=http://localhost:11434/v1/chat/completions
LLM_API_KEY=
LLM_MODEL=llama3

# database
DATABASE_PATH=data/qa_records.db

# server
HOST=0.0.0.0
PORT=8000

# optional: enable debug logging (defaults to false)
# DEBUG=true

# langfuse configuration
LANGFUSE_SECRET_KEY="sk-..."
LANGFUSE_PUBLIC_KEY="pk-..."
LANGFUSE_HOST="https://cloud.langfuse.com"      # for litellm (tracing)
LANGFUSE_PROJECT_ID="..."
