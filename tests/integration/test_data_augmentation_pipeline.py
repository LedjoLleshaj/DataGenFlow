"""End-to-end test for Data Augmentation pipeline template"""

import pytest

from lib.llm_config import LLMConfigManager
from lib.templates import template_registry
from lib.workflow import Pipeline as WorkflowPipeline


@pytest.mark.integration
@pytest.mark.asyncio
async def test_data_augmentation_pipeline_e2e_real(e2e_storage):
    """
    e2e test for data augmentation pipeline with real ollama calls

    tests full pipeline: StructureSampler -> SemanticInfiller -> DuplicateRemover
    verifies:
    - pipeline execution completes successfully
    - correct number of results generated
    - result structure matches expectations
    - all blocks executed with valid traces
    - usage tracking works
    - embedding-based duplicate detection runs
    """
    # initialize llm config manager with test storage
    llm_config_manager = LLMConfigManager(e2e_storage)

    # monkey patch the global llm_config_manager
    import app

    original_manager = app.llm_config_manager
    app.llm_config_manager = llm_config_manager

    try:
        # get template
        template = template_registry.get_template("data_augmentation")
        assert template is not None, "data_augmentation template not found"

        # create pipeline from template
        pipeline_def = {"name": "Test Data Augmentation E2E", "blocks": template["blocks"]}
        pipeline = WorkflowPipeline.load_from_dict(pipeline_def)

        # create seed data with minimal records (1 seed for speed)
        seed_data = {
            "target_count": 1,  # minimal for fast e2e test
            "categorical_fields": ["category"],
            "numeric_fields": ["price"],
            "dependencies": {},  # no dependencies for this simple example
            "fields_to_generate": ["description", "price"],
            "comparison_fields": ["description"],
            "samples": [
                {
                    "category": "electronics",
                    "price": 299,
                    "description": "Wireless noise-canceling headphones with premium sound quality",
                },
                {
                    "category": "furniture",
                    "price": 199,
                    "description": "Ergonomic office chair with lumbar support",
                },
            ],
        }

        # execute pipeline (batch returns single ExecutionResult)
        # this will make REAL calls to ollama
        print("\nðŸš€ Running e2e test with real LLM calls to Ollama...")
        execution_result = await pipeline.execute(seed_data)

        # verify return type and structure
        assert hasattr(execution_result, "result"), "Should be ExecutionResult dataclass"

        # extract fields from ExecutionResult
        result_data = execution_result.result
        trace = execution_result.trace
        trace_id = execution_result.trace_id
        usage = execution_result.usage

        # verify result has samples array
        assert "samples" in result_data, "Result should have samples array"
        samples = result_data["samples"]
        assert isinstance(samples, list), "samples should be a list"
        assert len(samples) > 0, "Should generate at least one sample"
        print(f"âœ… Generated {len(samples)} samples")

        # verify first sample structure
        first_sample = samples[0]
        assert "category" in first_sample, "Sample should have category field"
        assert "price" in first_sample, "Sample should have price field"
        assert "description" in first_sample, "Description should be generated by SemanticInfiller"

        # verify DuplicateRemover added dual similarity fields
        assert "is_duplicate" in first_sample, "DuplicateRemover should add is_duplicate flag"
        assert "similarity_to_seeds" in first_sample, (
            "DuplicateRemover should add similarity_to_seeds"
        )
        assert "similarity_to_generated" in first_sample, (
            "DuplicateRemover should add similarity_to_generated"
        )
        assert isinstance(first_sample["is_duplicate"], bool), "is_duplicate should be boolean"
        assert isinstance(first_sample["similarity_to_seeds"], (int, float)), (
            "similarity_to_seeds should be numeric"
        )
        assert isinstance(first_sample["similarity_to_generated"], (int, float)), (
            "similarity_to_generated should be numeric"
        )
        assert 0.0 <= first_sample["similarity_to_seeds"] <= 1.0, (
            "similarity_to_seeds should be in [0,1]"
        )
        assert 0.0 <= first_sample["similarity_to_generated"] <= 1.0, (
            "similarity_to_generated should be in [0,1]"
        )
        print(f"âœ… Sample structure valid: {list(first_sample.keys())}")

        # verify trace contains all blocks in batch pipeline
        assert len(trace) == 3, f"Should have 3 blocks in trace, got {len(trace)}"
        assert trace[0].block_type == "StructureSampler", "First block should be StructureSampler"
        assert trace[1].block_type == "SemanticInfiller", "Second block should be SemanticInfiller"
        assert trace[2].block_type == "DuplicateRemover", "Third block should be DuplicateRemover"
        print(f"âœ… All blocks executed: {[t.block_type for t in trace]}")

        # verify usage tracking
        assert usage.input_tokens > 0, "Should have input tokens from LLM calls"
        assert usage.output_tokens > 0, "Should have output tokens from LLM calls"
        print(f"âœ… Usage tracked: in={usage.input_tokens}, out={usage.output_tokens}")

        # verify trace_id exists
        assert trace_id is not None
        assert len(trace_id) > 0

        # verify generated content quality (basic sanity checks)
        description = first_sample.get("description", "")
        assert len(description) > 0, "Generated description should not be empty"

        price = first_sample.get("price")
        assert isinstance(price, (int, float)), "Generated price should be numeric"
        assert price > 0, "Generated price should be positive"

        print("\nâœ… E2E test passed!")
        print(f"ðŸ“Š Sample result: {first_sample}")
        print(f"ðŸ“ˆ Usage: in={usage.input_tokens}, out={usage.output_tokens}")

    finally:
        # restore original llm_config_manager
        app.llm_config_manager = original_manager


@pytest.mark.integration
@pytest.mark.asyncio
async def test_data_augmentation_pipeline_missing_fields_error():
    """test that pipeline fails with clear error when required fields are missing"""
    template = template_registry.get_template("data_augmentation")
    assert template is not None

    pipeline_def = {"name": "Test Data Augmentation", "blocks": template["blocks"]}
    pipeline = WorkflowPipeline.load_from_dict(pipeline_def)

    # seed data missing required template variables
    seed_data = {
        "samples": [
            {"plan": "free", "role": "user", "storage": 10, "bio": "Casual user"},
        ]
    }

    # should fail with undefined variable error
    with pytest.raises(Exception) as exc_info:
        await pipeline.execute(seed_data)

    error_msg = str(exc_info.value)
    assert "undefined" in error_msg.lower() or "target_count" in error_msg
